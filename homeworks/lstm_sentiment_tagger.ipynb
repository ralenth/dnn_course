{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task definition\nImplement LSTM Sentiment Tagger for imdb reviews dataset.\n\n1. (5pt) Fill missing code below\n    * 1pt implement vectorization\n    * 2pt implement \\_\\_init\\_\\_ and forward methods of models\n    * 2pt implement collate function\n2. (4pt) Implement training loop, choose proper loss function, use clear ml for max points.\n    * 2pts is a baseline for well written, working code\n    * 2pts if clear ml used properly\n3. (3pt) Train the models (find proper hyperparams). Make sure you are not overfitting or underfitting. Visualize training of your best model (plot training, and test loss/accuracy in time). Your model should reach at least 87% accuracy. For max points it should exceed 89%. \n    * 1pt for accuracy above 89%\n    * 1pt for accuracy above 87%\n    * 1pt for visualizations\n\nRemarks:\n* Use embeddings of size 50\n* Use 0.5 threshold when computing accuracy.\n* Use supplied dataset for training and evaluation.\n* You do not have to use validation set.\n* You should monitor overfitting during training.\n* For max points use clear ml to store and manage logs from your experiments. \n* We encourage to use pytorch lightning library (Addtional point for using it - however the sum must not exceed 12)\n\n[Clear ML documentation](https://clear.ml/docs/latest/docs/)\n\n[Clear ML notebook exercise from bootcamp](https://colab.research.google.com/drive/1wtLb4gg8beLS7smcyJlOZppn6_rQvSxL?usp=sharing)","metadata":{"id":"YhhKpOc0fl9Q"}},{"cell_type":"code","source":"device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:22:51.829673Z","iopub.execute_input":"2022-01-15T20:22:51.830110Z","iopub.status.idle":"2022-01-15T20:22:51.862046Z","shell.execute_reply.started":"2022-01-15T20:22:51.830016Z","shell.execute_reply":"2022-01-15T20:22:51.861334Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install clearml\n\nimport os\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torchtext\nfrom clearml import Task\n\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"IE-XdhV_b2nS","outputId":"36a72088-f36f-4001-89f5-d84a7be2d58d","execution":{"iopub.status.busy":"2022-01-15T20:22:51.864297Z","iopub.execute_input":"2022-01-15T20:22:51.864578Z","iopub.status.idle":"2022-01-15T20:23:05.082378Z","shell.execute_reply.started":"2022-01-15T20:22:51.864540Z","shell.execute_reply":"2022-01-15T20:23:05.081621Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\nweb_server = 'https://app.community.clear.ml'\napi_server = 'https://api.community.clear.ml'\nfiles_server = 'https://files.community.clear.ml'\naccess_key = ''#@param {type:\"string\"}\nsecret_key = ''#@param {type:\"string\"}\n\nTask.set_credentials(web_host=web_server,\n                     api_host=api_server,\n                     files_host=files_server,\n                     key=access_key,\n                     secret=secret_key)","metadata":{"id":"ilxUhICVWQqZ","execution":{"iopub.status.busy":"2022-01-15T21:55:38.786190Z","iopub.execute_input":"2022-01-15T21:55:38.786943Z","iopub.status.idle":"2022-01-15T21:55:38.791963Z","shell.execute_reply.started":"2022-01-15T21:55:38.786893Z","shell.execute_reply":"2022-01-15T21:55:38.791192Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\n!gdown https://drive.google.com/uc?id=1hK-3iiRPlbePb99Fe-34LJNZ5yB-nduq\n!tar -xvzf imdb_dataset.gz\ndata = pd.read_csv(\"imdb_dataset.csv\")","metadata":{"id":"iULzpw-rzKrA","outputId":"829b01c6-0d86-486d-ee83-5af223282015","execution":{"iopub.status.busy":"2022-01-15T20:23:05.092374Z","iopub.execute_input":"2022-01-15T20:23:05.092808Z","iopub.status.idle":"2022-01-15T20:23:32.692286Z","shell.execute_reply.started":"2022-01-15T20:23:05.092772Z","shell.execute_reply":"2022-01-15T20:23:32.691459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nfrom keras.preprocessing.sequence import pad_sequences\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:23:32.700858Z","iopub.execute_input":"2022-01-15T20:23:32.701197Z","iopub.status.idle":"2022-01-15T20:23:37.223895Z","shell.execute_reply.started":"2022-01-15T20:23:32.701156Z","shell.execute_reply":"2022-01-15T20:23:37.223013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:23:37.237033Z","iopub.execute_input":"2022-01-15T20:23:37.237510Z","iopub.status.idle":"2022-01-15T20:23:38.406835Z","shell.execute_reply.started":"2022-01-15T20:23:37.237474Z","shell.execute_reply":"2022-01-15T20:23:38.406116Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class NaiveVectorizer:\n    def __init__(self, tokenized_data, **kwargs):\n        \"\"\"Converts data from string to vector of ints that represent words. \n        Prepare lookup dict (self.wv) that maps token to int. Reserve index 0 for padding.\n        \"\"\"\n        tokenized_data = self._clean_text(tokenized_data)\n        tokenized_data = [seq.split() for seq in tokenized_data]\n        self.wv = self._init_wv(tokenized_data)\n        \n    \n    def _clean_text(self, data):\n        \"\"\"Remove special signs, leaving only letters, digits and spaces.\"\"\"\n        return [re.sub('[^A-Za-z0-9\\s]+', '', seq) for seq in data]\n\n    def _init_wv(self, tokenized_data):\n        \"\"\"Initialize token dictionary.\"\"\"\n        tokenized_data = [item for sublist in tokenized_data for item in sublist]\n        tokenized_data = set(tokenized_data)\n        non_stopwords = list(set(tokenized_data).difference(set(stopwords.words(\"english\"))))\n        return {token: i + 1 for i, token in enumerate(non_stopwords)}\n\n\n    def vectorize(self, tokenized_seq):\n        \"\"\"Converts sequence of tokens into sequence of indices.\n        If the token does not appear in the vocabulary(self.wv) it is ommited\n        Returns torch tensor of shape (seq_len,) and type long.\"\"\"\n        tokenized_seq = self._clean_text(tokenized_seq)\n        converted_seq = [self.wv[token] for token in tokenized_seq if token in self.wv.keys()]\n        return torch.tensor(converted_seq)\n\n\nclass ImdbDataset(Dataset):\n    SPLIT_TYPES = [\"train\", \"test\", \"unsup\"]\n\n    def __init__(self, data, preprocess_fn, split=\"train\"):\n        super(ImdbDataset, self).__init__()\n        if split not in self.SPLIT_TYPES:\n            raise AttributeError(f\"No such split type: {split}\")\n\n        self.split = split\n        self.label = [i for i, c in enumerate(data.columns) if c == \"sentiment\"][0]\n        self.data_col = [i for i, c in enumerate(data.columns) if c == \"tokenized\"][0]\n        self.data = data[data[\"split\"] == self.split]\n        self.preprocess_fn = preprocess_fn\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.preprocess_fn(self.data.iloc[idx, self.data_col].split())\n        label = self.data.iloc[idx, self.label]\n        return (seq, label)\n\n\nnaive_vectorizer = NaiveVectorizer(data.loc[data[\"split\"] == \"train\", \"tokenized\"])\n\n\ndef get_datasets():\n    train_dataset = ImdbDataset(data, naive_vectorizer.vectorize)\n    test_dataset = ImdbDataset(data, naive_vectorizer.vectorize, split=\"test\")\n        \n    return train_dataset, test_dataset\n\n\ndef custom_collate_fn(pairs):\n    \"\"\"This function is supposed to be used by dataloader to prepare batches\n    Input: list of tuples (sequence, label)\n    Output: sequences_padded_to_the_same_lenths, original_lenghts_of_sequences, lables.\n    torch.nn.utils.rnn.pad_sequence might be usefull here\n    \"\"\"\n    seqcs = [pair[0] for pair in pairs]\n    labels = [pair[1] for pair in pairs]\n    lengths = torch.tensor([len(seq) for seq in seqcs])\n    seqcs = torch.nn.utils.rnn.pad_sequence(seqcs, batch_first=True)\n    \n    return seqcs, lengths, torch.tensor(labels).float()","metadata":{"id":"sisjcg2gtIdR","execution":{"iopub.status.busy":"2022-01-15T20:23:48.133104Z","iopub.execute_input":"2022-01-15T20:23:48.133569Z","iopub.status.idle":"2022-01-15T20:23:51.032812Z","shell.execute_reply.started":"2022-01-15T20:23:48.133531Z","shell.execute_reply":"2022-01-15T20:23:51.031998Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:08:02.936265Z","iopub.execute_input":"2022-01-15T22:08:02.936979Z","iopub.status.idle":"2022-01-15T22:08:04.045188Z","shell.execute_reply.started":"2022-01-15T22:08:02.936942Z","shell.execute_reply":"2022-01-15T22:08:04.044329Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"\"\"\"Implement LSTMSentimentTagger. \nThe model should use a LSTM module.\nUse torch.nn.utils.rnn.pack_padded_sequence to optimize processing of sequences.\nWhen computing vocab_size of embedding layer remeber that padding_symbol counts to the vocab.\nUse sigmoid activation function.\n\"\"\"\nclass LSTMSentimentTagger(torch.nn.Module):\n    def __init__(self, vocab_dim, hidden_dim, batch_size, num_layers=1, device='cuda'):\n        super(LSTMSentimentTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        \n        self.embed = torch.nn.Embedding(vocab_dim + 1, 50, padding_idx=0)\n        self.layernorm1 = torch.nn.LayerNorm(50)\n\n        self.lstm = torch.nn.LSTM(\n            50, self.hidden_dim, num_layers, batch_first=True, bidirectional=False, dropout=0.2\n        )\n        self.hidden = self.init_hidden(batch_size, num_layers, device)\n        self.layernorm2 = torch.nn.LayerNorm(self.hidden_dim * num_layers)\n        \n        self.dropout = torch.nn.Dropout(0.25)\n        \n        self.fc1 = torch.nn.Linear(self.hidden_dim * num_layers, 1)\n\n    def init_hidden(self, batch_size, num_layers, device='cuda'):\n        return (torch.autograd.Variable(torch.zeros(num_layers, batch_size, self.hidden_dim).to(device)),\n                torch.autograd.Variable(torch.zeros(num_layers, batch_size, self.hidden_dim).to(device)))\n\n    def forward(self, x, lengths):\n        x = self.embed(x)\n        x = self.dropout(x)\n        x = self.layernorm1(x)\n        x = torch.nn.utils.rnn.pack_padded_sequence(\n            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        _, (hn, _) = self.lstm(x, self.hidden)\n        x = hn.permute(1, 0, 2)\n        x = x.reshape(self.batch_size, -1)\n        x = self.layernorm2(x)\n\n        x = self.dropout(x)\n        x = self.fc1(x)\n        return torch.sigmoid(x)","metadata":{"id":"KJjHVS8riqWZ","execution":{"iopub.status.busy":"2022-01-15T20:59:09.850247Z","iopub.execute_input":"2022-01-15T20:59:09.850526Z","iopub.status.idle":"2022-01-15T20:59:09.867622Z","shell.execute_reply.started":"2022-01-15T20:59:09.850493Z","shell.execute_reply":"2022-01-15T20:59:09.866646Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# Trainig loop and visualizations\n","metadata":{"id":"aOvWoFyNwPBS"}},{"cell_type":"code","source":"def train(model, train_loader, optimizer, loss_f, device='cuda'):\n    model.train()\n    train_loss = []\n    correct = 0\n    \n    for x in train_loader:\n        data, lengths, target = x\n        data, lengths, target = data.to(device), lengths.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data, lengths).flatten().float()\n        pred = torch.tensor([1 if pred_i >= 0.5 else 0 for pred_i in output]).to(device)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        loss = loss_f(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n    \n    return np.array([\n        np.mean(train_loss), correct / len(train_loader.dataset)\n    ])","metadata":{"id":"bgjiOXsrzmly","execution":{"iopub.status.busy":"2022-01-15T22:00:31.933085Z","iopub.execute_input":"2022-01-15T22:00:31.933362Z","iopub.status.idle":"2022-01-15T22:00:31.944302Z","shell.execute_reply.started":"2022-01-15T22:00:31.933331Z","shell.execute_reply":"2022-01-15T22:00:31.941772Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader, loss_f, device='cuda'):\n    model.eval()\n    test_loss = []\n    correct = 0\n\n    with torch.no_grad():\n        for x in test_loaders:\n            data, lengths, target = x\n            data, lengths, target = data.to(device), lengths.to(device), target.to(device)\n            output = model(data, lengths).flatten().float()\n            test_loss.append(loss_f(output, target).item())\n            pred = torch.tensor([1 if pred_i >= 0.5 else 0 for pred_i in output]).to(device)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss = np.mean(test_loss)\n    return np.array([test_loss, correct / len(test_loader.dataset)])","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:00:32.211778Z","iopub.execute_input":"2022-01-15T22:00:32.212370Z","iopub.status.idle":"2022-01-15T22:00:32.219385Z","shell.execute_reply.started":"2022-01-15T22:00:32.212340Z","shell.execute_reply":"2022-01-15T22:00:32.218647Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"train_dataset, test_dataset = get_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:15.058448Z","iopub.execute_input":"2022-01-15T22:12:15.058753Z","iopub.status.idle":"2022-01-15T22:12:15.132628Z","shell.execute_reply.started":"2022-01-15T22:12:15.058722Z","shell.execute_reply":"2022-01-15T22:12:15.131799Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"config = {\n    'seed': 256,\n    'n_epochs': 20,\n    'lr': 1e-3,\n    'patience': 3,\n    'batch_size': 500\n}","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:21.793971Z","iopub.execute_input":"2022-01-15T22:12:21.794253Z","iopub.status.idle":"2022-01-15T22:12:21.798799Z","shell.execute_reply.started":"2022-01-15T22:12:21.794182Z","shell.execute_reply":"2022-01-15T22:12:21.797943Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    dataset=train_dataset, batch_size=config['batch_size'], shuffle=True, \n    collate_fn=custom_collate_fn,\n)\ntest_loader = DataLoader(\n    dataset=test_dataset, batch_size=config['batch_size'], shuffle=False, \n    collate_fn=custom_collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:21.800498Z","iopub.execute_input":"2022-01-15T22:12:21.800747Z","iopub.status.idle":"2022-01-15T22:12:21.812708Z","shell.execute_reply.started":"2022-01-15T22:12:21.800715Z","shell.execute_reply":"2022-01-15T22:12:21.812020Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from clearml import Task\n\n\ntask = Task.create(project_name='gsn', task_name='lstm_tagger')\ntask.mark_started()\nlogger = task.get_logger()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:21.814750Z","iopub.execute_input":"2022-01-15T22:12:21.815086Z","iopub.status.idle":"2022-01-15T22:12:23.599186Z","shell.execute_reply.started":"2022-01-15T22:12:21.815053Z","shell.execute_reply":"2022-01-15T22:12:23.598452Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(config['seed'])\nmodel = LSTMSentimentTagger(len(naive_vectorizer.wv), 32, config['batch_size'], num_layers=2).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config['patience'])\nloss_f = torch.nn.BCELoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:23.600445Z","iopub.execute_input":"2022-01-15T22:12:23.600692Z","iopub.status.idle":"2022-01-15T22:12:23.653693Z","shell.execute_reply.started":"2022-01-15T22:12:23.600658Z","shell.execute_reply":"2022-01-15T22:12:23.652859Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"for i in range(config['n_epochs']):\n    train_loss, train_acc = train(model, train_loader, optimizer, loss_f)\n    test_loss, test_acc = test(model, test_loader, loss_f)\n    print((f'{i:3} | train loss: {train_loss:.5f}, train_acc: {train_acc:.3f}, '\n           f'test loss: {test_loss:.5f} test_acc: {test_acc:.3f}'))\n    \n    logger.report_scalar(title='Loss', series='Train', iteration=i, value=train_loss)\n    logger.report_scalar(title='Loss', series='Test', iteration=i, value=test_loss)\n    logger.report_scalar(title='Accuracy', series='Train', iteration=i, value=train_acc)\n    logger.report_scalar(title='Accuracy', series='Test', iteration=i, value=test_acc)\n\n    if scheduler:\n        scheduler.step(test_loss)\n\n\ntask.mark_completed()\ntask.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:12:23.655818Z","iopub.execute_input":"2022-01-15T22:12:23.656308Z","iopub.status.idle":"2022-01-15T22:25:00.961060Z","shell.execute_reply.started":"2022-01-15T22:12:23.656246Z","shell.execute_reply":"2022-01-15T22:25:00.960326Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:25:00.963022Z","iopub.execute_input":"2022-01-15T22:25:00.963310Z","iopub.status.idle":"2022-01-15T22:25:01.819797Z","shell.execute_reply.started":"2022-01-15T22:25:00.963258Z","shell.execute_reply":"2022-01-15T22:25:01.819094Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"task = Task.get_tasks(\n    project_name='gsn',\n    tags=['best']\n)[0]\n\nmetrics = task.get_reported_scalars()\n\nfig = px.line(pd.DataFrame({'train': metrics['Accuracy']['Train']['y'], \n                            'test': metrics['Accuracy']['Test']['y']}),\n              labels={'index': 'Epochs',\n                      'value': 'Accuracy',\n                      'variable': 'Dataset'},\n              title='Accuracy')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:28:40.320779Z","iopub.execute_input":"2022-01-15T22:28:40.321064Z","iopub.status.idle":"2022-01-15T22:28:41.822457Z","shell.execute_reply.started":"2022-01-15T22:28:40.321034Z","shell.execute_reply":"2022-01-15T22:28:41.821753Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"fig = px.line(pd.DataFrame({'train': metrics['Loss']['Train']['y'], \n                            'test': metrics['Loss']['Test']['y']}),\n              labels={'index': 'Epochs',\n                      'value': 'Loss',\n                      'variable': 'Dataset'},\n              title='Loss')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:28:58.214224Z","iopub.execute_input":"2022-01-15T22:28:58.214899Z","iopub.status.idle":"2022-01-15T22:28:58.282143Z","shell.execute_reply.started":"2022-01-15T22:28:58.214861Z","shell.execute_reply":"2022-01-15T22:28:58.281480Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}